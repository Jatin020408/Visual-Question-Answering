{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUQ82jLwfCzp"
   },
   "source": [
    "###Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2506,
     "status": "ok",
     "timestamp": 1651200828673,
     "user": {
      "displayName": "Diene Newrone",
      "userId": "11499617903453027359"
     },
     "user_tz": 240
    },
    "id": "TH6oy-ElfHev",
    "outputId": "b336aa29-d982-40a1-e0d7-cdc93cdf3e60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1651200828673,
     "user": {
      "displayName": "Diene Newrone",
      "userId": "11499617903453027359"
     },
     "user_tz": 240
    },
    "id": "_IbzjAPGfPrB",
    "outputId": "2aae3f2b-c936-484d-c4d0-320a109ad732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/vqa\n"
     ]
    }
   ],
   "source": [
    "# Since we've shared this drive with you, please use the correct file path from your drive since it'll go to SharedDrive for you\n",
    "%cd '/content/drive/MyDrive/vqa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeENDeNpLPbb"
   },
   "source": [
    "###Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1651200832062,
     "user": {
      "displayName": "Diene Newrone",
      "userId": "11499617903453027359"
     },
     "user_tz": 240
    },
    "id": "EWPWCQJ9LM7W"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from PIL import Image\n",
    "import re\n",
    "import time\n",
    "import cv2\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBtSbtFJMQ9U"
   },
   "source": [
    "###Setting the constants and parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1651200833462,
     "user": {
      "displayName": "Diene Newrone",
      "userId": "11499617903453027359"
     },
     "user_tz": 240
    },
    "id": "4kyUxL5EMVqW"
   },
   "outputs": [],
   "source": [
    "input_dir = '/content/drive/MyDrive/vqa'\n",
    "log_dir = '/content/drive/MyDrive/vqa/logs'\n",
    "model_dir = '/content/drive/MyDrive/vqa/models'\n",
    "\n",
    "# maximum length of question, the length in the VQA dataset is 26\n",
    "max_qst_length = 30\n",
    "# maximum number of answers\n",
    "max_num_ans = 10\n",
    "# embedding size of feature vector for image and question\n",
    "embed_size = 1024\n",
    "# embedding size of the word used as the input for the LSTM\n",
    "word_embed_size = 300\n",
    "# Number of layers in the LSTM\n",
    "num_layers = 2\n",
    "# Hidden size in the LSTM\n",
    "hidden_size = 64\n",
    "# Learning rate, step size and decay rate used while initializing the Step learning rate Scheduler\n",
    "learning_rate = 0.001\n",
    "step_size = 10\n",
    "gamma = 0.1\n",
    "#Number of epochs it is trained on\n",
    "num_epochs = 30\n",
    "#Batch size, number of workers and the steps after which the model parameters are saved\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "save_step = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byQqyFa2LyJ4"
   },
   "source": [
    "###Helper Functions for Handling Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1651200835463,
     "user": {
      "displayName": "Diene Newrone",
      "userId": "11499617903453027359"
     },
     "user_tz": 240
    },
    "id": "nO1Aqs0QLsWM"
   },
   "outputs": [],
   "source": [
    "SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
    "\n",
    "# create tokens\n",
    "def tokenize(sentence):\n",
    "    tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n",
    "    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n",
    "    return tokens\n",
    "\n",
    "# returns a file as list of lines\n",
    "def load_str_list(fname):\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "# Tokenizes the text and then gives the index of the word from the vocab txt file of answers and questions\n",
    "class VocabDict:\n",
    "\n",
    "    def __init__(self, vocab_file):\n",
    "        self.word_list = load_str_list(vocab_file)\n",
    "        self.word2idx_dict = {w:n_w for n_w, w in enumerate(self.word_list)}\n",
    "        self.vocab_size = len(self.word_list)\n",
    "        self.unk2idx = self.word2idx_dict['<unk>'] if '<unk>' in self.word2idx_dict else None\n",
    "\n",
    "    def idx2word(self, n_w):\n",
    "\n",
    "        return self.word_list[n_w]\n",
    "\n",
    "    def word2idx(self, w):\n",
    "        if w in self.word2idx_dict:\n",
    "            return self.word2idx_dict[w]\n",
    "        elif self.unk2idx is not None:\n",
    "            return self.unk2idx\n",
    "        else:\n",
    "            raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n",
    "\n",
    "    def tokenize_and_index(self, sentence):\n",
    "        inds = [self.word2idx(w) for w in tokenize(sentence)]\n",
    "\n",
    "        return inds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMKzQIZZMCyh"
   },
   "source": [
    "###Building the Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1651200838572,
     "user": {
      "displayName": "Diene Newrone",
      "userId": "11499617903453027359"
     },
     "user_tz": 240
    },
    "id": "j2ri8KxPL3xH"
   },
   "outputs": [],
   "source": [
    "class VqaDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, input_dir, input_vqa, max_qst_length=30, max_num_ans=10, transform=None):\n",
    "        self.input_dir = input_dir\n",
    "        self.vqa = np.load(input_dir+'/'+input_vqa, allow_pickle=True)\n",
    "        self.qst_vocab = VocabDict(input_dir+'/dataset/vocab_questions.txt')\n",
    "        self.ans_vocab = VocabDict(input_dir+'/dataset/vocab_answers.txt')\n",
    "        self.max_qst_length = max_qst_length\n",
    "        self.max_num_ans = max_num_ans\n",
    "        self.load_ans = ('valid_answers' in self.vqa[0]) and (self.vqa[0]['valid_answers'] is not None)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        vqa = self.vqa\n",
    "        qst_vocab = self.qst_vocab\n",
    "        ans_vocab = self.ans_vocab\n",
    "        max_qst_length = self.max_qst_length\n",
    "        max_num_ans = self.max_num_ans\n",
    "        transform = self.transform\n",
    "        load_ans = self.load_ans\n",
    "\n",
    "        image = vqa[idx]['image_path']\n",
    "        image = Image.open(image).convert('RGB')\n",
    "        qst2idc = np.array([qst_vocab.word2idx('<pad>')] * max_qst_length)  # padded with '<pad>' in 'ans_vocab'\n",
    "        qst2idc[:len(vqa[idx]['question_tokens'])] = [qst_vocab.word2idx(w) for w in vqa[idx]['question_tokens']]\n",
    "        sample = {'image': image, 'question': qst2idc}\n",
    "\n",
    "        if load_ans:\n",
    "            ans2idc = [ans_vocab.word2idx(w) for w in vqa[idx]['valid_answers']]\n",
    "            ans2idx = np.random.choice(ans2idc)\n",
    "            sample['answer_label'] = ans2idx         # for training\n",
    "\n",
    "            mul2idc = list([-1] * max_num_ans)       # padded with -1 (no meaning) not used in 'ans_vocab'\n",
    "            mul2idc[:len(ans2idc)] = ans2idc         # our model should not predict -1\n",
    "            sample['answer_multi_choice'] = mul2idc  # for evaluation metric\n",
    "\n",
    "        if transform:\n",
    "            sample['image'] = transform(sample['image'])\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.vqa)\n",
    "\n",
    "\n",
    "def get_loader(input_dir, input_vqa_train, input_vqa_valid, max_qst_length, max_num_ans, batch_size, num_workers):\n",
    "\n",
    "    transform = {\n",
    "        phase: transforms.Compose([transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                        (0.229, 0.224, 0.225))]) \n",
    "        for phase in ['train', 'valid']}\n",
    "\n",
    "    vqa_dataset = {\n",
    "        'train': VqaDataset(\n",
    "            input_dir=input_dir,\n",
    "            input_vqa=input_vqa_train,\n",
    "            max_qst_length=max_qst_length,\n",
    "            max_num_ans=max_num_ans,\n",
    "            transform=transform['train']),\n",
    "        'valid': VqaDataset(\n",
    "            input_dir=input_dir,\n",
    "            input_vqa=input_vqa_valid,\n",
    "            max_qst_length=max_qst_length,\n",
    "            max_num_ans=max_num_ans,\n",
    "            transform=transform['valid'])}\n",
    "\n",
    "    data_loader = {\n",
    "        phase: torch.utils.data.DataLoader(\n",
    "            dataset=vqa_dataset[phase],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers)\n",
    "        for phase in ['train', 'valid']}\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kubGfT_R0yQ"
   },
   "source": [
    "##Model: Image Encoding Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1651200842039,
     "user": {
      "displayName": "Diene Newrone",
      "userId": "11499617903453027359"
     },
     "user_tz": 240
    },
    "id": "3crpWchKR1Ua"
   },
   "outputs": [],
   "source": [
    "class ImgAttentionEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "\n",
    "        super(ImgAttentionEncoder, self).__init__()\n",
    "        vggnet_feat = models.vgg19(pretrained=True).features\n",
    "        modules = list(vggnet_feat.children())[:-2]\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "        self.fc = nn.Sequential(nn.Linear(self.cnn[-3].out_channels, embed_size),\n",
    "                                nn.Tanh())     # feature vector of image\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_feature = self.cnn(image)    \n",
    "        #print(img_feature.shape)                       # [batch_size, vgg16(19)_fc=4096]\n",
    "        img_feature = img_feature.view(-1, 512, 196).transpose(1,2) # [batch_size, 196, 512]\n",
    "        img_feature = self.fc(img_feature)                          # [batch_size, 196, embed_size]\n",
    "        #print(image.shape, img_feature.shape,\"\\n\")\n",
    "\n",
    "        return img_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RS-9L7-_SmpP"
   },
   "source": [
    "###Model: Question Encoding Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1651200844822,
     "user": {
      "displayName": "Diene Newrone",
      "userId": "11499617903453027359"
     },
     "user_tz": 240
    },
    "id": "InpvRJdcSqPD"
   },
   "outputs": [],
   "source": [
    "class QstEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size):\n",
    "\n",
    "        super(QstEncoder, self).__init__()\n",
    "        self.word2vec = nn.Embedding(qst_vocab_size, word_embed_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.lstm = nn.LSTM(word_embed_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(2*num_layers*hidden_size, embed_size)     # 2 for hidden and cell states\n",
    "\n",
    "    def forward(self, question):\n",
    "\n",
    "        qst_vec = self.word2vec(question)                             # [batch_size, max_qst_length=30, word_embed_size=300]\n",
    "        qst_vec = self.tanh(qst_vec)\n",
    "        qst_vec = qst_vec.transpose(0, 1)                             # [max_qst_length=30, batch_size, word_embed_size=300]\n",
    "        _, (hidden, cell) = self.lstm(qst_vec)                        # [num_layers=2, batch_size, hidden_size=512]\n",
    "        qst_feature = torch.cat((hidden, cell), 2)                    # [num_layers=2, batch_size, 2*hidden_size=1024]\n",
    "        qst_feature = qst_feature.transpose(0, 1)                     # [batch_size, num_layers=2, 2*hidden_size=1024]\n",
    "        qst_feature = qst_feature.reshape(qst_feature.size()[0], -1)  # [batch_size, 2*num_layers*hidden_size=2048]\n",
    "        qst_feature = self.tanh(qst_feature)\n",
    "        qst_feature = self.fc(qst_feature)                            # [batch_size, embed_size]\n",
    "\n",
    "        return qst_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAxCHSplZyvG"
   },
   "source": [
    "###Model: Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1651200847410,
     "user": {
      "displayName": "Diene Newrone",
      "userId": "11499617903453027359"
     },
     "user_tz": 240
    },
    "id": "mTUQNgeSZ1ZG"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, num_channels, embed_size, dropout=True):\n",
    "\n",
    "        super(Attention, self).__init__()\n",
    "        self.ff_image = nn.Linear(embed_size, num_channels)\n",
    "        self.ff_questions = nn.Linear(embed_size, num_channels)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.ff_attention = nn.Linear(num_channels, 1)\n",
    "\n",
    "    def forward(self, vi, vq):\n",
    "\n",
    "        hi = self.ff_image(vi)\n",
    "        hq = self.ff_questions(vq).unsqueeze(dim=1)\n",
    "        ha = torch.tanh(hi+hq)\n",
    "        if self.dropout:\n",
    "            ha = self.dropout(ha)\n",
    "        ha = self.ff_attention(ha)\n",
    "        pi = torch.softmax(ha, dim=1)\n",
    "        self.pi = pi\n",
    "        vi_attended = (pi * vi).sum(dim=1)    # creating the weighted image vector using attention distribution\n",
    "        u = vi_attended + vq      # concatenating the new query vector and the weighted image vector\n",
    "        return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIFmwwSJS_9s"
   },
   "source": [
    "###Model: Combine Image Encoding and Question Encoding Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 2282,
     "status": "ok",
     "timestamp": 1651200851039,
     "user": {
      "displayName": "Diene Newrone",
      "userId": "11499617903453027359"
     },
     "user_tz": 240
    },
    "id": "C57q-LSBTE9g"
   },
   "outputs": [],
   "source": [
    "class SANModel(nn.Module):\n",
    "    def __init__(self, embed_size, qst_vocab_size, ans_vocab_size, word_embed_size, num_layers, hidden_size): \n",
    "        super(SANModel, self).__init__()\n",
    "        self.num_attention_layer = 2\n",
    "        self.num_mlp_layer = 1\n",
    "        self.img_encoder = ImgAttentionEncoder(embed_size)\n",
    "        self.qst_encoder = QstEncoder(qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size)\n",
    "        self.san = nn.ModuleList([Attention(512, embed_size)]*self.num_attention_layer)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mlp = nn.Sequential(nn.Dropout(p=0.5),\n",
    "                            nn.Linear(embed_size, ans_vocab_size))\n",
    "        self.attn_features = []  ## attention features\n",
    "\n",
    "    def forward(self, img, qst):\n",
    "\n",
    "        img_feature = self.img_encoder(img)                     # [batch_size, embed_size]\n",
    "        qst_feature = self.qst_encoder(qst)                     # [batch_size, embed_size]\n",
    "        vi = img_feature\n",
    "        u = qst_feature\n",
    "        for attn_layer in self.san:\n",
    "            u = attn_layer(vi, u)\n",
    "            \n",
    "        combined_feature = self.mlp(u)\n",
    "        return combined_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2JJKjmqPBTM"
   },
   "source": [
    "###Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auJoIXFiPAhE"
   },
   "outputs": [],
   "source": [
    "# Get data loader for train and test - it's a dictionary with the key train having the train dataloader and same for test\n",
    "data_loader = get_loader(\n",
    "        input_dir=input_dir,\n",
    "        input_vqa_train='dataset/train.npy',\n",
    "        input_vqa_valid='dataset/valid.npy',\n",
    "        max_qst_length=max_qst_length,\n",
    "        max_num_ans=max_num_ans,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers)\n",
    "\n",
    "qst_vocab_size = data_loader['train'].dataset.qst_vocab.vocab_size\n",
    "ans_vocab_size = data_loader['train'].dataset.ans_vocab.vocab_size\n",
    "ans_unk_idx = data_loader['train'].dataset.ans_vocab.unk2idx\n",
    "\n",
    "# Initializing the model\n",
    "model = SANModel(\n",
    "        embed_size=embed_size,\n",
    "        qst_vocab_size=qst_vocab_size,\n",
    "        ans_vocab_size=ans_vocab_size,\n",
    "        word_embed_size=word_embed_size,\n",
    "        num_layers=num_layers,\n",
    "        hidden_size=hidden_size).to(device)\n",
    "\n",
    "# Initializing the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initializing the optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "\n",
    "last_time = 0\n",
    "early_stop_threshold = 3\n",
    "best_loss = 99999\n",
    "val_increase_count = 0\n",
    "stop_training = False\n",
    "prev_loss = 9999\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for phase in ['train', 'valid']:\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corr = 0\n",
    "\n",
    "        batch_step_size = len(data_loader[phase].dataset) / batch_size\n",
    "\n",
    "        if phase == 'train':\n",
    "            scheduler.step()\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for batch_idx, batch_sample in enumerate(data_loader[phase]):\n",
    "            \n",
    "            image = batch_sample['image'].to(device)\n",
    "            question = batch_sample['question'].to(device)\n",
    "            label = batch_sample['answer_label'].to(device)\n",
    "            multi_choice = batch_sample['answer_multi_choice']  # not tensor, list.\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                output = model(image, question)      # size: [batch_size X ans_vocab_size=1000]\n",
    "                _, pred = torch.max(output, 1)  # size: [batch_size]\n",
    "\n",
    "                loss = criterion(output, label)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # Evaluation metric \n",
    "            running_loss += loss.item()\n",
    "            running_corr += torch.stack([(ans == pred.cpu()) for ans in multi_choice]).any(dim=0).sum()\n",
    "\n",
    "            # Print the average loss in a mini-batch.\n",
    "            if batch_idx % 10 == 0:\n",
    "                time_taken = time.time() - last_time\n",
    "                time_left = (((batch_step_size - batch_idx) * time_taken)/10) * (num_epochs - epoch)\n",
    "                print('| {} SET | Epoch [{:02d}/{:02d}], Step [{:04d}/{:04d}], Loss: {:.4f}, Time left: {:.2f} hr'\n",
    "                      .format(phase.upper(), epoch+1, num_epochs, batch_idx, int(batch_step_size), loss.item(), time_left/3600))\n",
    "                last_time = time.time()\n",
    "        # Print the average loss and accuracy in an epoch.\n",
    "        epoch_loss = running_loss / batch_step_size\n",
    "        epoch_acc = running_corr.double() / len(data_loader[phase].dataset)      \n",
    "\n",
    "        print('| {} SET | Epoch [{:02d}/{:02d}], Loss: {:.4f}, Acc: {:.4f}\\n'\n",
    "              .format(phase.upper(), epoch+1, num_epochs, epoch_loss, epoch_acc))\n",
    "\n",
    "        \n",
    "\n",
    "        # Log the loss and accuracy in an epoch.\n",
    "        with open(os.path.join(log_dir, '{}-log-epoch-{:02}.txt')\n",
    "                  .format(phase, epoch+1), 'w') as f:\n",
    "            f.write(str(epoch+1) + '\\t'\n",
    "                    + str(epoch_loss) + '\\t'\n",
    "                    + str(epoch_acc.item()))\n",
    "            \n",
    "        if phase == 'valid':\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                torch.save(model, os.path.join(model_dir, 'best_model.pt'))\n",
    "            if epoch_loss > prev_loss:\n",
    "                val_increase_count += 1\n",
    "            else:\n",
    "                val_increase_count = 0\n",
    "            if val_increase_count >= early_stop_threshold:\n",
    "                stop_training = True\n",
    "            prev_loss = epoch_loss\n",
    "\n",
    "    # Save the model check points.\n",
    "    if (epoch+1) % save_step == 0:\n",
    "        torch.save(model, os.path.join(model_dir, '-epoch-{:02d}.pt'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ya37IFIqTca8"
   },
   "source": [
    "###Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1651200896403,
     "user": {
      "displayName": "Diene Newrone",
      "userId": "11499617903453027359"
     },
     "user_tz": 240
    },
    "id": "-v5XEy_EUs2U"
   },
   "outputs": [],
   "source": [
    "image_path = '/content/drive/MyDrive/vqa/dataset/Resized_Images/test_img.jpeg'\n",
    "question = 'what does the sign say?'\n",
    "saved_model = '/content/drive/MyDrive/vqa/models/best_model.pt'\n",
    "max_qst_length=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQvOBxL4T1BW"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "qst_vocab = load_str_list(\"/content/drive/MyDrive/vqa/dataset/vocab_questions.txt\")\n",
    "ans_vocab = load_str_list(\"/content/drive/MyDrive/vqa/dataset/vocab_answers.txt\")\n",
    "word2idx_dict = {w:n_w for n_w, w in enumerate(qst_vocab)}\n",
    "unk2idx = word2idx_dict['<unk>'] if '<unk>' in word2idx_dict else None\n",
    "qst_vocab_size = len(qst_vocab)\n",
    "ans_vocab_size = len(ans_vocab)\n",
    "\n",
    "\n",
    "def word2idx(w):\n",
    "        if w in word2idx_dict:\n",
    "            return word2idx_dict[w]\n",
    "        elif unk2idx is not None:\n",
    "            return unk2idx\n",
    "        else:\n",
    "            raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n",
    "\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.resize(image, dsize=(224,224), interpolation = cv2.INTER_AREA)\n",
    "image = torch.from_numpy(image).float()\n",
    "image = image.to(device)\n",
    "image = image.unsqueeze(dim=0)\n",
    "image = image.view(1,3,224,224)\n",
    "max_qst_length=30\n",
    "\n",
    "try:\n",
    "  q_list = list(question.split(\" \"))\n",
    "except:\n",
    "  q_list = list(question.split(1))\n",
    "idx = 'valid'\n",
    "qst2idc = np.array([word2idx('<pad>')] * max_qst_length)  # padded with '<pad>' in 'ans_vocab'\n",
    "qst2idc[:len(q_list)] = [word2idx(w) for w in q_list]\n",
    "\n",
    "question = qst2idc\n",
    "question = torch.from_numpy(question).long()\n",
    "\n",
    "question = question.to(device)\n",
    "question = question.unsqueeze(dim=0)\n",
    "\n",
    "net = torch.load(saved_model)\n",
    "net = net.to(device)\n",
    "\n",
    "net.eval()\n",
    "output = model(image, question)\n",
    "\n",
    "\n",
    "predicts = torch.softmax(output, 1)\n",
    "probs, indices = torch.topk(predicts, k=5, dim=1)\n",
    "probs = probs.squeeze()\n",
    "indices = indices.squeeze()\n",
    "print(\"predicted - probabilty\")\n",
    "for i in range(5):\n",
    "\n",
    "  print(\"'{}' - {:.4f}\".format(ans_vocab[indices[i].item()], probs[i].item()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Advanced_DL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
